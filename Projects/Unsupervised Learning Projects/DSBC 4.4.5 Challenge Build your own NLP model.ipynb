{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of A Study In Scarlet, by Arthur Conan Doyle\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: A Study In Scarlet\n",
      "\n",
      "Author: Arthur Conan Doyle\n",
      "\n",
      "Posting Date: July 12, 2008 [EBook #244]\n",
      "Release Date: April, 1995\n",
      "Last Updated: September 30, 2016\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK A STUDY IN SCARLET ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Roger Squires\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A STUDY IN SCARLET.\n",
      "\n",
      "By A. Conan Doyle\n",
      "\n",
      "[1]\n",
      "\n",
      "\n",
      "\n",
      "     Original Transcriber’s Note: This etext is prepared directly\n",
      "     from an 1887 edition, and care has been taken to duplicate the\n",
      "     original exactly, including typographical and punctuation\n",
      "     vagaries.\n",
      "\n",
      "     Additions to the text include adding the underscore character to\n",
      "     indicate italics, and textual end\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of Eminent Victorians, by Lytton Strachey\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Eminent Victorians\n",
      "\n",
      "Author: Lytton Strachey\n",
      "\n",
      "Posting Date: February 21, 2012 [EBook #2447]\n",
      "Release Date: December, 2000\n",
      "[Last updated: August 19, 2012]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK EMINENT VICTORIANS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by E-Text prepared by Martin Adamson;\n",
      "martin@grassmarket.freeserve.co.uk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EMINENT VICTORIANS\n",
      "\n",
      "by Lytton Strachey\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Preface\n",
      "\n",
      "THE history of the Victorian Age will never be written; we know too much\n",
      "about it. For ignorance is the first requisite of the historian--ignorance,\n",
      "which simplifies and clarifies, which selects and omits, with a placid\n",
      "perfection unattainable by the highest art. Concerning the Age whi\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of Knights of Art, by Amy Steedman\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "Title: Knights of Art\n",
      "       Stories of the Italian Painters\n",
      "\n",
      "Author: Amy Steedman\n",
      "\n",
      "Posting Date: September 13, 2008 [EBook #529]\n",
      "Release Date: May, 1996\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK KNIGHTS OF ART ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Charles Keller.  HTML version by Al Haines.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KNIGHTS OF ART\n",
      "\n",
      "STORIES OF THE ITALIAN PAINTERS\n",
      "\n",
      "\n",
      "BY AMY STEEDMAN\n",
      "\n",
      "AUTHOR OF 'IN GOD'S GARDEN'\n",
      "\n",
      "\n",
      "\n",
      "TO FRANCESCA\n",
      "\n",
      "\n",
      "\n",
      "ABOUT THIS BOOK\n",
      "\n",
      "What would we do without our picture-books, I wonder? Before we knew\n",
      "how to read, before even we could speak, we had learned to love them.\n",
      "We shouted with pleasure when we turned the pages and saw the spotted\n",
      "cow standing in the daisy-sprinkled mead\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg eBook, Prince Henry the Navigator, the Hero of\n",
      "Portugal and of Modern Discovery, 1394-1460 A.D., by C. Raymond Beazley\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: Prince Henry the Navigator, the Hero of Portugal and of Modern Discovery, 1394-1460 A.D.\n",
      "       With an Account of Geographical Progress Throughout the Middle Ages As the Preparation for His Work.\n",
      "\n",
      "\n",
      "Author: C. Raymond Beazley\n",
      "\n",
      "\n",
      "\n",
      "Release Date: July 4, 2006  [eBook #18757]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "***START OF THE PROJECT GUTENBERG EBOOK PRINCE HENRY THE NAVIGATOR, THE\n",
      "HERO OF PORTUGAL AND OF MODERN DISCOVERY, 1394-1460 A.D.***\n",
      "\n",
      "\n",
      "E-text prepared by Suzanne Lybarger and the Project Gutenberg Online\n",
      "Distributed Proofreading Team (http://www.pgdp.net/)\n",
      "\n",
      "\n",
      "\n",
      "Note: Project Gutenberg also has an HTML ve\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿Project Gutenberg’s Real Soldiers of Fortune, by Richard Harding Davis\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Real Soldiers of Fortune\n",
      "\n",
      "Author: Richard Harding Davis\n",
      "\n",
      "Posting Date: February 22, 2009 [EBook #3029]\n",
      "Last Updated: September 26, 2016\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK REAL SOLDIERS OF FORTUNE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by David Reed, and Ronald J. Wilson\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REAL SOLDIERS OF FORTUNE\n",
      "\n",
      "\n",
      "By Richard Harding Davis\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MAJOR-GENERAL HENRY RONALD DOUGLAS MACIVER\n",
      "\n",
      "ANY sunny afternoon, on Fifth Avenue, or at night in the _table d’hote_\n",
      "restaurants of University Place, you may meet the soldier of fortune who\n",
      "of all his brothers in arms now living is the most remarkable. You may\n",
      "have noticed him; a stiffly erect, disti\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of Saint Augustin, by Louis Bertrand\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.  If you are not located in the United States, you'll have\n",
      "to check the laws of the country where you are located before using this ebook.\n",
      "\n",
      "Title: Saint Augustin\n",
      "\n",
      "Author: Louis Bertrand\n",
      "\n",
      "Posting Date: September 25, 2014 [EBook #9069]\n",
      "Release Date: October, 2005\n",
      "First Posted: September 2, 2003\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK SAINT AUGUSTIN ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Charles Aldorondo, Tiffany Vergon, William\n",
      "Flis, and Distributed Proofreaders\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SAINT AUGUSTIN\n",
      "\n",
      "BY\n",
      "\n",
      "LOUIS BERTRAND\n",
      "\n",
      "\n",
      "TRANSLATED BY VINCENT O'SULLIVAN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TRANSLATOR'S NOTE\n",
      "\n",
      "\n",
      "The quotations from Saint Augustin's _Confessions_ are\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of Sense and Sensibility, by Jane Austen\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Sense and Sensibility\n",
      "\n",
      "Author: Jane Austen\n",
      "\n",
      "Commentator: Austin Dobson\n",
      "\n",
      "Illustrator: Hugh Thomson\n",
      "\n",
      "Release Date: June 15, 2007 [EBook #21839]\n",
      "[Last updated: February 11, 2015]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK SENSE AND SENSIBILITY ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Fritz Ohrenschall and Sankar Viswanathan (This\n",
      "book was produced from scanned images of public domain\n",
      "material from the Google Print project.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                         Transcriber's Note:\n",
      "\n",
      "The Table of Contents is not part of the original book. The illustration\n",
      "on page 290 is missing from the book. The Introduction ends abruptly.\n",
      "Seems incomplete.\n",
      "\n",
      "\n",
      "       [Illustration: _Mr. Da\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of Seven Wives and Seven Prisons, by L.A. Abbott\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Seven Wives and Seven Prisons\n",
      "\n",
      "Author: L.A. Abbott\n",
      "\n",
      "Release Date: November, 2003  [Etext #4667]\n",
      "Posting Date: January 27, 2010\n",
      "Last Updated: October 27, 2016\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK SEVEN WIVES AND SEVEN PRISONS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Charles Aldarondo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SEVEN WIVES AND SEVEN PRISONS\n",
      "\n",
      "Or Experiences In The Life Of A Matrimonial Maniac. A True Story.\n",
      "Written By Himself.\n",
      "\n",
      "\n",
      "By L.A. Abbott\n",
      "\n",
      "\n",
      "New York:\n",
      "\n",
      "Published For The Author. 1870.\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "CHAPTER 1. THE FIRST AND WORST WIFE My Early History. The First\n",
      "Marriage. Leaving Home to Prospect. Sending for My Wife. Her Mysterious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journey. W\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of The Ball and The Cross, by G.K. Chesterton\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: The Ball and The Cross\n",
      "\n",
      "Author: G.K. Chesterton\n",
      "\n",
      "Release Date: March, 2004 [EBook #5265]\n",
      "Posting Date: March 24, 2009\n",
      "Last Updated: March 9, 2018\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK THE BALL AND THE CROSS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Ben Crowder\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE BALL AND THE CROSS\n",
      "\n",
      "G.K. Chesterton\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "    I. A Discussion Somewhat in the Air\n",
      "   II. The Religion of the Stipendiary Magistrate\n",
      "  III. Some Old Curiosities\n",
      "   IV. A Discussion at Dawn\n",
      "    V. The Peacemaker\n",
      "   VI. The Other Philosopher\n",
      "  VII. The Village of Grassley-in-the-Hole\n",
      " VIII. An Interlude of Argument\n",
      "   IX. The Strange Lady\n",
      "    X. The Sword\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of The Life of Froude, by Herbert Paul\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "Title: The Life of Froude\n",
      "\n",
      "Author: Herbert Paul\n",
      "\n",
      "Release Date: February 9, 2005 [EBook #14992]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK THE LIFE OF FROUDE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Michael Madden\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Life of Froude\n",
      "\n",
      "By Herbert Paul\n",
      "\n",
      "London: Sir Isaac Pitman & Sons, 1905.\n",
      "\n",
      "\n",
      "PREFACE\n",
      "\n",
      "Although eleven years have elapsed since Mr. Froude's death, no\n",
      "biography of him has, so far as I know, appeared. This book is an\n",
      "attempt to tell the public something about a man whose writings have\n",
      "a permanent place in the literature of England.\n",
      "\n",
      "It is a pleasure to acknowledge my obligation to Miss Margaret\n",
      "Froude for having allowed me the use of such written material as\n",
      "\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg eBook, The Parent's Assistant, by Maria Edgeworth,\n",
      "Illustrated by F. A. Fraser\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions \n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "the Project Gutenberg License included with this eBook or online at \n",
      "www.gutenberg.org.  If you are not located in the United States, you'll have\n",
      "to check the laws of the country where you are located before using this ebook.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: The Parent's Assistant\n",
      "\n",
      "\n",
      "Author: Maria Edgeworth\n",
      "\n",
      "\n",
      "\n",
      "Release Date: March 17, 2015  [eBook #3655]\n",
      "[This file was first posted on July 3, 2001]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "\n",
      "***START OF THE PROJECT GUTENBERG EBOOK THE PARENT'S ASSISTANT***\n",
      "\n",
      "\n",
      "This eBook was produced by Les Bowler.\n",
      "\n",
      "                          [Picture: Book cover]\n",
      "\n",
      "                          [Picture: The orphans]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                   T\n",
      "\n",
      "Raw ********************************:\n",
      " ﻿The Project Gutenberg EBook of William Lloyd Garrison, by Archibald H. Grimke\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "Title: William Lloyd Garrison\n",
      "       The Abolitionist\n",
      "\n",
      "Author: Archibald H. Grimke\n",
      "\n",
      "Release Date: January 1, 2005 [EBook #14555]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK WILLIAM LLOYD GARRISON ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Jonathan Ingram, Amy Overmyer and the Online Distributed\n",
      "Proofreading Team\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Illustration: Wm. Lloyd Garrison]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WILLIAM LLOYD GARRISON\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_THE ABOLITIONIST_\n",
      "\n",
      "BY ARCHIBALD H. GRIMKE, M.A.\n",
      "\n",
      "\n",
      "\n",
      "Library of Congress Cataloging in Publication Data\n",
      "\n",
      "Grimké, Archibald Henry, 1849-1930.\n",
      "\n",
      "William Lloyd Garrison, the abolitionist.\n",
      "\n",
      "Reprint of the 1891 ed. published by Funk & Wagnalls, New York.\n",
      "\n",
      "1. Garrison, William Lloyd, 1805-1879\n"
     ]
    }
   ],
   "source": [
    "f1 = open('A Study In Scarlet by Arthur Conan Doyle.txt','r+',encoding='UTF-8')\n",
    "scarlet = f1.read()\n",
    "# f2 = open('''Alice's Adventures in Wonderland by Lewis Carroll.txt''','r+',encoding='UTF-8')\n",
    "# alice = f2.read()\n",
    "f3 = open('Eminent Victorians by Lytton Strachey.txt','r+',encoding='UTF-8')\n",
    "victorians = f3.read()\n",
    "f4 = open('Knights of Art by Amy Steedman.txt','r+',encoding='UTF-8')\n",
    "knights = f4.read()\n",
    "f5 = open('Prince Henry the Navigator by Raymond Beazley.txt','r+',encoding='UTF-8')\n",
    "navigator = f5.read()\n",
    "f6 = open('Real Soldiers of Fortune by Richard Harding Davis.txt','r+',encoding='UTF-8')\n",
    "soldiers = f6.read()\n",
    "f7 = open('Saint Augustin by Louis Bertrand.txt','r+',encoding='UTF-8')\n",
    "augustin = f7.read()\n",
    "f8 = open('Sense and Sensibility by Jane Austen.txt','r+',encoding='UTF-8')\n",
    "sense = f8.read()\n",
    "f9 = open('Seven Wives and Seven Prisons by LA Abbott.txt','r+',encoding='UTF-8')\n",
    "wives = f9.read()\n",
    "f10 = open('The Ball and the Cross by G. K. Chesterton.txt','r+',encoding='UTF-8')\n",
    "ball = f10.read()\n",
    "f11 = open('The life of Froude by Herbert Paul.txt','r+',encoding='UTF-8')\n",
    "froude = f11.read()\n",
    "f12 = open('''The Parent's Assistant by Maria Edgeworth.txt''','r+',encoding='UTF-8')\n",
    "parents = f12.read()\n",
    "f13 = open('William Lloyd Garrison The Abolitionist by Archibald Grimke.txt','r+',encoding='UTF-8')\n",
    "abolitionist = f13.read()\n",
    "\n",
    "titles = [scarlet,victorians,knights,navigator,soldiers,augustin,sense,wives,\n",
    "          ball,froude,parents,abolitionist]\n",
    "for title in titles:\n",
    "    print('\\nRaw ********************************:\\n', title[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all texts that we downloaded from gutenberg.org have more stuff at each beginning than raw books in gutenberg pacakge. Including titles, preface, table of contents, author biography,index, appendix, gutenberg project-relative info, and some legal disclamation. I think we can just cut them off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw ********************************:\n",
      " \n",
      "PART I.\n",
      "\n",
      "(_Being a reprint from the reminiscences of_ JOHN H. WATSON, M.D., _late\n",
      "of the Army Medical Department._) [2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I. MR. SHERLOCK HOLMES.\n",
      "\n",
      "\n",
      "IN the year 1878 I took my degree of Doctor of Medicine of the\n",
      "University of London, and proceeded to Netley to go through the course\n",
      "prescribed for surgeons in the army. Having completed my studies there,\n",
      "I was duly attached to the Fifth Northumberland Fusiliers as Assistant\n",
      "Surgeon. The regiment was stationed in India at the time, and before\n",
      "I could join it, the second Afghan war had broken out. On landing at\n",
      "Bombay, I learned that my corps had advanced through the passes, and\n",
      "was already deep in the enemy’s country. I followed, however, with many\n",
      "other officers who were in the same situation as myself, and succeeded\n",
      "in reaching Candahar in safety, where I found my regiment, and at once\n",
      "entered upon my new duties.\n",
      "\n",
      "The campaign brought honours and promotion to many, but for me it had\n",
      "nothing but misfortune and disaster. I was r\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "UNDOUBTEDLY, what is most obviously striking in the history of Manning's\n",
      "career is the persistent strength of his innate characteristics. Through\n",
      "all the changes of his fortunes the powerful spirit of the man worked on\n",
      "undismayed. It was as if the Fates had laid a wager that they would\n",
      "daunt him; and in the end they lost their bet.\n",
      "\n",
      "His father was a rich West Indian merchant, a governor of the Bank of\n",
      "England, a Member of Parliament, who drove into town every day from his\n",
      "country seat in a coach and four, and was content with nothing short of\n",
      "a bishop for the christening of his children. Little Henry, like the\n",
      "rest, had his bishop; but he was obliged to wait for him--for as long as\n",
      "eighteen months. In those days, and even a generation later, as Keble\n",
      "bears witness, there was great laxity in regard to the early baptism of\n",
      "children. The delay has been noted by Manning's biographer as the first\n",
      "stumbling-block in the spiritual life of the future Cardinal; but he\n",
      "surmounted it with s\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "GIOTTO\n",
      "\n",
      "It was more than six hundred years ago that a little peasant baby was\n",
      "born in the small village of Vespignano, not far from the beautiful\n",
      "city of Florence, in Italy. The baby's father, an honest, hard-working\n",
      "countryman, was called Bondone, and the name he gave to his little son\n",
      "was Giotto.\n",
      "\n",
      "Life was rough and hard in that country home, but the peasant baby grew\n",
      "into a strong, hardy boy, learning early what cold and hunger meant.\n",
      "The hills which surrounded the village were grey and bare, save where\n",
      "the silver of the olive-trees shone in the sunlight, or the tender\n",
      "green of the shooting corn made the valley beautiful in early spring.\n",
      "In summer there was little shade from the blazing sun as it rode high\n",
      "in the blue sky, and the grass which grew among the grey rocks was\n",
      "often burnt and brown. But, nevertheless, it was here that the sheep of\n",
      "the village would be turned out to find what food they could, tended\n",
      "and watched by one of the village boys.\n",
      "\n",
      "So it happened that when Gio\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "INTRODUCTION.\n",
      "\n",
      "THE GREEK AND ARABIC IDEAS OF THE WORLD, AS THE CHIEF INHERITANCE OF THE\n",
      "CHRISTIAN MIDDLE AGES IN GEOGRAPHICAL KNOWLEDGE.\n",
      "\n",
      "\n",
      "Arabic science constitutes one of the main links between the older\n",
      "learned world of the Greeks and Latins and the Europe of Henry the\n",
      "Navigator and of the Renaissance. In geography it adopted in the main\n",
      "the results of Ptolemy and Strabo; and many of the Moslem travellers and\n",
      "writers gained some additional hints from Indian, Persian, and Chinese\n",
      "knowledge; but, however much of fact they added to Greek cartography,\n",
      "they did not venture to correct its postulates.\n",
      "\n",
      "And what were these postulates? In part, they were the assumptions of\n",
      "modern draughtsmen, but in some important details they differed. And\n",
      "first, as to agreement. Three continents, Europe, Asia, and Africa, an\n",
      "encircling ocean, the Mediterranean, the Black Sea and Caspian, the Red\n",
      "Sea and Persian Gulf, the South Asiatic, and North and West European\n",
      "coasts were indicated with more or less \n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "MAJOR-GENERAL HENRY RONALD DOUGLAS MACIVER\n",
      "\n",
      "ANY sunny afternoon, on Fifth Avenue, or at night in the _table d’hote_\n",
      "restaurants of University Place, you may meet the soldier of fortune who\n",
      "of all his brothers in arms now living is the most remarkable. You may\n",
      "have noticed him; a stiffly erect, distinguished-looking man, with gray\n",
      "hair, an imperial of the fashion of Louis Napoleon, fierce blue eyes,\n",
      "and across his forehead a sabre cut.\n",
      "\n",
      "This is Henry Ronald Douglas MacIver, for some time in India an ensign\n",
      "in the Sepoy mutiny; in Italy, lieutenant under Garibaldi; in Spain,\n",
      "captain under Don Carlos; in our Civil War, major in the Confederate\n",
      "army; in Mexico, lieutenant-colonel under the Emperor Maximilian;\n",
      "colonel under Napoleon III, inspector of cavalry for the Khedive of\n",
      "Egypt, and chief of cavalry and general of brigade of the army of King\n",
      "Milan of Servia. These are only a few of his military titles. In 1884\n",
      "was published a book giving the story of his life up to that year. It\n",
      "was\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "SAINT AUGUSTIN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "\n",
      "\n",
      "  Inquietum est cor nostrum donec requiescat in te.\n",
      "  \"Our heart finds no rest until it rests in Thee.\"\n",
      "\n",
      "  _Confessions_, I, i.\n",
      "\n",
      "\n",
      "Saint Augustin is now little more than a celebrated name. Outside of\n",
      "learned or theological circles people no longer read him. Such is true\n",
      "renown: we admire the saints, as we do great men, on trust. Even his\n",
      "_Confessions_ are generally spoken of only from hearsay. By this neglect,\n",
      "is he atoning for the renewal of glory in which he shone during the\n",
      "seventeenth century, when the Jansenists, in their inveterate obstinacy,\n",
      "identified him with the defence of their cause? The reputation of sour\n",
      "austerity and of argumentative and tiresome prolixity which attaches to\n",
      "the remembrance of all the writers of Port-Royal, save Pascal--has that\n",
      "affected too the work of Augustin, enlisted in spite of himself in the\n",
      "ranks of these pious schismatics? And yet, if there have ever been any\n",
      "beings who do not resemble Augustin, and whom probably he \n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex. Their estate\n",
      "was large, and their residence was at Norland Park, in the centre of\n",
      "their property, where, for many generations, they had lived in so\n",
      "respectable a manner as to engage the general good opinion of their\n",
      "surrounding acquaintance. The late owner of this estate was a single\n",
      "man, who lived to a very advanced age, and who for many years of his\n",
      "life, had a constant companion and housekeeper in his sister. But her\n",
      "death, which happened ten years before his own, produced a great\n",
      "alteration in his home; for to supply her loss, he invited and\n",
      "received into his house the family of his nephew Mr. Henry Dashwood,\n",
      "the legal inheritor of the Norland estate, and the person to whom he\n",
      "intended to bequeath it. In the society of his nephew and niece, and\n",
      "their children, the old Gentleman's days were comfortably spent. His\n",
      "attachment to them all increased. The constant attention of Mr. and\n",
      "Mrs. Henry Dashwood to his wishes\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "SEVEN WIVES AND SEVEN PRISONS\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I. THE FIRST AND WORST WIFE\n",
      "\n",
      "My Early History--THE FIRST MARRIAGE--LEAVING HOME TO PROSPECT--SENDING\n",
      "FOR MY WIFE--HER MYSTERIOUS JOURNEY--WHERE I FOUND HER--TEN DOLLARS FOR\n",
      "NOTHING--A FASCINATING HOTEL CLERK--MY WIFE’S CONFESSION--FROM BAD TO\n",
      "WORSE--FINAL SEPARATION--TRIAL FOR FORGERY--A PRIVATE MARRIAGE--SUMMARY\n",
      "SEPARATION.\n",
      "\n",
      "\n",
      "\n",
      "SOME one has said that if any man would faithfully write his\n",
      "autobiography, giving truly his own history and experiences, the ills\n",
      "and joys, the haps and mishaps that had fallen to his lot, he could not\n",
      "fail to make an interesting story; and Disraeli makes Sidonia say\n",
      "that there is romance in every life. How much romance, as well as sad\n",
      "reality, there is in the life of a man who, among other experiences,\n",
      "has married seven wives, and has been seven times in prison--solely on\n",
      "account of the seven wives, may be learned from the pages that follow.\n",
      "\n",
      "I was born in the town of Chatham, Columbia County, New York, in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September, \n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "I. A DISCUSSION SOMEWHAT IN THE AIR\n",
      "\n",
      "The flying ship of Professor Lucifer sang through the skies like a\n",
      "silver arrow; the bleak white steel of it, gleaming in the bleak\n",
      "blue emptiness of the evening. That it was far above the earth was no\n",
      "expression for it; to the two men in it, it seemed to be far above the\n",
      "stars. The professor had himself invented the flying machine, and had\n",
      "also invented nearly everything in it. Every sort of tool or apparatus\n",
      "had, in consequence, to the full, that fantastic and distorted look\n",
      "which belongs to the miracles of science. For the world of science and\n",
      "evolution is far more nameless and elusive and like a dream than the\n",
      "world of poetry and religion; since in the latter images and ideas\n",
      "remain themselves eternally, while it is the whole idea of evolution\n",
      "that identities melt into each other as they do in a nightmare.\n",
      "\n",
      "All the tools of Professor Lucifer were the ancient human tools gone\n",
      "mad, grown into unrecognizable shapes, forgetful of their origin,\n",
      "for\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "CHAPTER I\n",
      "\n",
      "CHILDHOOD\n",
      "\n",
      "IN reading biographies I always skip the genealogical details. To\n",
      "be born obscure and to die famous has been described as the acme of\n",
      "human felicity. However that may be, whether fame has anything to do\n",
      "with happiness or no, it is a man himself, and not his ancestors,\n",
      "whose life deserves, if it does deserve, to be written. Such was\n",
      "Froude's own opinion, and it is the opinion of most sensible people.\n",
      "Few, indeed, are the families which contain more than one remarkable\n",
      "figure, and this is the rock upon which the hereditary principle\n",
      "always in practice breaks. For human lineage is not subject to the\n",
      "scientific tests which alone could give it solid value as positive\n",
      "or negative evidence. There is nothing to show from what source,\n",
      "other than the ultimate source of every good and perfect gift,\n",
      "Froude derived his brilliant and splendid powers. He was a gentleman,\n",
      "and he did not care to find or make for himself a pedigree. He knew\n",
      "that the Froudes had been settled in Dev\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "THE ORPHANS.\n",
      "\n",
      "\n",
      "NEAR the ruins of the castle of Rossmore, in Ireland, is a small cabin,\n",
      "in which there once lived a widow and her four children.  As long as she\n",
      "was able to work, she was very industrious, and was accounted the best\n",
      "spinner in the parish; but she overworked herself at last, and fell ill,\n",
      "so that she could not sit to her wheel as she used to do, and was obliged\n",
      "to give it up to her eldest daughter, Mary.\n",
      "\n",
      "Mary was at this time about twelve years old.  One evening she was\n",
      "sitting at the foot of her mother’s bed spinning, and her little brothers\n",
      "and sisters were gathered round the fire eating their potatoes and milk\n",
      "for supper.  “Bless them, the poor young creatures!” said the widow, who,\n",
      "as she lay on her bed, which she knew must be her deathbed, was thinking\n",
      "of what would become of her children after she was gone.  Mary stopped\n",
      "her wheel, for she was afraid that the noise of it had wakened her\n",
      "mother, and would hinder her from going to sleep again.\n",
      "\n",
      "“No need to stop t\n",
      "\n",
      "Raw ********************************:\n",
      " \n",
      "\n",
      "\n",
      "CHAPTER I.\n",
      "\n",
      "\n",
      "THE FATHER OF THE MAN.\n",
      "\n",
      "\n",
      "William Lloyd Garrison was born in Newburyport, Massachusetts, December\n",
      "10, 1805. Forty years before, Daniel Palmer, his great-grandfather,\n",
      "emigrated from Massachusetts and settled with three sons and a daughter\n",
      "on the St. John River, in Nova Scotia. The daughter's name was Mary, and\n",
      "it was she who was to be the future grandmother of our hero. One of the\n",
      "neighbors of Daniel Palmer was Joseph Garrison, who was probably an\n",
      "Englishman. He was certainly a bachelor. The Acadian solitude of five\n",
      "hundred acres and Mary Palmer's charms proved too much for the\n",
      "susceptible heart of Joseph Garrison. He wooed and won her, and on his\n",
      "thirtieth birthday she became his wife. The bride herself was but\n",
      "twenty-three, a woman of resources and of presence of mind, as she\n",
      "needed to be in that primitive settlement. Children and cares came apace\n",
      "to the young wife, and we may be sure confined her more and more closely\n",
      "to her house. But in the midst of a fast-increasing\n"
     ]
    }
   ],
   "source": [
    "#Cutting off the head and tail of each book, where it contains all kind of other info instead of \n",
    "#book content itself. Those numbers are found by using len(bookname), print(bookname[number:]) and\n",
    "#print(bookname[0:number])\n",
    "\n",
    "scarlet = scarlet[1775:238100]\n",
    "#alice = alice[710:143948]\n",
    "victorians = victorians[7902:567210]\n",
    "knights = knights[9151:245115]\n",
    "navigator = navigator[33220:476500]\n",
    "soldiers = soldiers[697:257290]\n",
    "augustin = augustin[4073:586130]\n",
    "sense = sense[17742:670740]\n",
    "wives = wives[6970:215402]\n",
    "ball = ball[1260:441365]\n",
    "froude = froude[2495:633470]\n",
    "parents = parents[9316:913058]\n",
    "abolitionist = abolitionist[3950:612780]\n",
    "\n",
    "titles = [scarlet,victorians,knights,navigator,soldiers,augustin,sense,wives,\n",
    "          ball,froude,parents,abolitionist]\n",
    "for title in titles:\n",
    "    print('\\nRaw ********************************:\\n', title[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's clean up some common patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "scarlet = re.sub(pattern, \"\", scarlet)\n",
    "victorians = re.sub(pattern, \"\", victorians)\n",
    "knights = re.sub(pattern, \"\", knights)\n",
    "navigator = re.sub(pattern, \"\", navigator)\n",
    "soldiers = re.sub(pattern, \"\", soldiers)\n",
    "augustin = re.sub(pattern, \"\", augustin)\n",
    "sense = re.sub(pattern, \"\", sense)\n",
    "wives = re.sub(pattern, \"\", wives)\n",
    "ball = re.sub(pattern, \"\", ball)\n",
    "froude = re.sub(pattern, \"\", froude)\n",
    "parents = re.sub(pattern, \"\", parents)\n",
    "abolitionist = re.sub(pattern, \"\", abolitionist)\n",
    "\n",
    "scarlet = re.sub(r'CHAPTER.*', \"\", scarlet)\n",
    "victorians = re.sub(r'CHAPTER.*', \"\", victorians)\n",
    "knights = re.sub(r'CHAPTER.*', \"\", knights)\n",
    "navigator = re.sub(r'CHAPTER.*', \"\", navigator)\n",
    "soldiers = re.sub(r'CHAPTER.*', \"\", soldiers)\n",
    "augustin = re.sub(r'CHAPTER.*', \"\", augustin)\n",
    "sense = re.sub(r'CHAPTER.*', \"\", sense)\n",
    "wives = re.sub(r'CHAPTER.*', \"\", wives)\n",
    "ball = re.sub(r'CHAPTER.*', \"\", ball)\n",
    "froude = re.sub(r'CHAPTER.*', \"\", froude)\n",
    "parents = re.sub(r'CHAPTER.*', \"\", parents)\n",
    "abolitionist = re.sub(r'CHAPTER.*', \"\", abolitionist)\n",
    "\n",
    "#######################################################################################################\n",
    "#Now, let's clean up each book one by one\n",
    "\n",
    "scarlet = re.sub('[]*()_[-]', '', scarlet) # Brackets\n",
    "scarlet = re.sub('PART.*', '', scarlet) # Title with words 'Part'\n",
    "\n",
    "victorians = re.sub('[]*()_[-]','', victorians) # Brackets\n",
    "victorians = re.sub('\\n[MDCLXVI]+\\n','', victorians,re.MULTILINE)  # Roman Numerals\n",
    "\n",
    "knights = re.sub('[]*()_[-]','', knights) # Brackets\n",
    "knights = re.sub('\\n[A-Z\\s]+\\n','', knights) # Capital titles\n",
    "\n",
    "navigator = re.sub('[]*()_[-]','', navigator) # Brackets\n",
    "navigator = re.sub('\\n.*[A-Z][.].*\\n','', navigator)  # Capital titles\n",
    "navigator = re.sub('Footnote .*:','',navigator) #Footnote with numbers\n",
    "navigator = re.sub('\\n\\d','',navigator) # Number in front of footnotes\n",
    "\n",
    "soldiers = re.sub('[]*()_[-]','', soldiers) # Brackets\n",
    "soldiers = re.sub('\\n[A-Z].*\\n\\n','', soldiers) # Capital Titles\n",
    "\n",
    "augustin = re.sub('[]*()_[-]','', augustin) # Brackets\n",
    "augustin = re.sub('\\n[MDCLXVI]+\\n','', augustin) # Roman Numerals\n",
    "augustin = re.sub('\\n[A-Z].*[A-Z]\\n\\n','', augustin)# Capital titles\n",
    "augustin = re.sub('\\n[A-Z].*[A-Z]\\n','', augustin) # Titles in different form\n",
    "augustin = re.sub('Confessions.*','', augustin) # Lines that repetive\n",
    "\n",
    "sense = re.sub('[]*()_[-]','', sense) # Brackets\n",
    "\n",
    "wives = re.sub('.*[-][-].*[A-Z][-][-].*','', wives) # Capital title in the form [-][-][A-Z][-][-]\n",
    "wives = re.sub('\\n.*[A-Z][.]\\n\\n','', wives) # Residuals from the last function\n",
    "wives = re.sub('[]*()_[-]','', wives) # Rest brackets\n",
    "wives = wives[35:]  # Simply cut off the title\n",
    "\n",
    "ball = re.sub('\\n[MDCLXVI]+[.].*\\n\\n','', ball) # Title for each chapter\n",
    "ball = re.sub('[]*()_[-]','', ball) # Brackets\n",
    "\n",
    "froude = re.sub('[-].*\\n[*].*\\n.*','', froude) # Appendix footnote\n",
    "froude = re.sub('\\n[A-Z].*\\n\\n','', froude) # Capital letter Titles\n",
    "froude = re.sub('[]*()_[-]','', froude) # Brackets\n",
    "\n",
    "parents = re.sub('\\n[A-Z].*[.]\\n\\n','', parents) # Capital letter Titles\n",
    "parents = re.sub('[]*()_[-]','', parents) # Brackets\n",
    "\n",
    "abolitionist = re.sub('\\n[A-Z]+[ ].*\\n\\n','', abolitionist) # Capital letter Titles\n",
    "abolitionist = re.sub('[]*()_[-]','', abolitionist) # Brackets\n",
    "\n",
    "#Clean up all numeral characters\n",
    "scarlet = re.sub(r'\\d', \"\", scarlet)\n",
    "victorians = re.sub(r'\\d', \"\", victorians)\n",
    "knights = re.sub(r'\\d', \"\", knights)\n",
    "navigator = re.sub(r'\\d', \"\", navigator)\n",
    "soldiers = re.sub(r'\\d', \"\", soldiers)\n",
    "augustin = re.sub(r'\\d', \"\", augustin)\n",
    "sense = re.sub(r'\\d', \"\", sense)\n",
    "wives = re.sub(r'\\d', \"\", wives)\n",
    "ball = re.sub(r'\\d', \"\", ball)\n",
    "froude = re.sub(r'\\d', \"\", froude)\n",
    "parents = re.sub(r'\\d', \"\", parents)\n",
    "abolitionist = re.sub(r'\\d', \"\", abolitionist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw ********************************:\n",
      " being a reprint from the reminiscences of john h. watson, m.d., late of the army medical department. in the year i took my degree of doctor of medicine of the university of london, and proceeded to netley to go through the course prescribed for surgeons in the army. having completed my studies there, i was duly attached to the fifth northumberland fusiliers as assistant surgeon. the regiment was stationed in india at the time, and before i could join it, the second afghan war had broken out. on landing at bombay, i learned that my corps had advanced through the passes, and was already deep in the enemy’s country. i followed, however, with many other officers who were in the same situation as myself, and succeeded in reaching candahar in safety, where i found my regiment, and at once entered upon my new duties. the campaign brought honours and promotion to many, but for me it had nothing but misfortune and disaster. i was removed from my brigade and attached to the berkshires, with whom\n",
      "\n",
      "Raw ********************************:\n",
      " undoubtedly, what is most obviously striking in the history of manning's career is the persistent strength of his innate characteristics. through all the changes of his fortunes the powerful spirit of the man worked on undismayed. it was as if the fates had laid a wager that they would daunt him; and in the end they lost their bet. his father was a rich west indian merchant, a governor of the bank of england, a member of parliament, who drove into town every day from his country seat in a coach and four, and was content with nothing short of a bishop for the christening of his children. little henry, like the rest, had his bishop; but he was obliged to wait for himfor as long as eighteen months. in those days, and even a generation later, as keble bears witness, there was great laxity in regard to the early baptism of children. the delay has been noted by manning's biographer as the first stumblingblock in the spiritual life of the future cardinal; but he surmounted it with success. hi\n",
      "\n",
      "Raw ********************************:\n",
      " it was more than six hundred years ago that a little peasant baby was born in the small village of vespignano, not far from the beautiful city of florence, in italy. the baby's father, an honest, hardworking countryman, was called bondone, and the name he gave to his little son was giotto. life was rough and hard in that country home, but the peasant baby grew into a strong, hardy boy, learning early what cold and hunger meant. the hills which surrounded the village were grey and bare, save where the silver of the olivetrees shone in the sunlight, or the tender green of the shooting corn made the valley beautiful in early spring. in summer there was little shade from the blazing sun as it rode high in the blue sky, and the grass which grew among the grey rocks was often burnt and brown. but, nevertheless, it was here that the sheep of the village would be turned out to find what food they could, tended and watched by one of the village boys. so it happened that when giotto was ten year\n",
      "\n",
      "Raw ********************************:\n",
      " the greek and arabic ideas of the world, as the chief inheritance of the arabic science constitutes one of the main links between the older learned world of the greeks and latins and the europe of henry the navigator and of the renaissance. in geography it adopted in the main the results of ptolemy and strabo; and many of the moslem travellers and writers gained some additional hints from indian, persian, and chinese knowledge; but, however much of fact they added to greek cartography, they did not venture to correct its postulates. and what were these postulates? in part, they were the assumptions of modern draughtsmen, but in some important details they differed. and first, as to agreement. three continents, europe, asia, and africa, an encircling ocean, the mediterranean, the black sea and caspian, the red sea and persian gulf, the south asiatic, and north and west european coasts were indicated with more or less precision in the science of the antonines and even of hannibal's age. \n",
      "\n",
      "Raw ********************************:\n",
      " any sunny afternoon, on fifth avenue, or at night in the table d’hote restaurants of university place, you may meet the soldier of fortune who of all his brothers in arms now living is the most remarkable. you may have noticed him; a stiffly erect, distinguishedlooking man, with gray hair, an imperial of the fashion of louis napoleon, fierce blue eyes, and across his forehead a sabre cut. this is henry ronald douglas maciver, for some time in india an ensign in the sepoy mutiny; in italy, lieutenant under garibaldi; in spain, captain under don carlos; in our civil war, major in the confederate army; in mexico, lieutenantcolonel under the emperor maximilian; colonel under napoleon iii, inspector of cavalry for the khedive of egypt, and chief of cavalry and general of brigade of the army of king milan of servia. these are only a few of his military titles. in was published a book giving the story of his life up to that year. it was called “under fourteen flags.” if today general maciver \n",
      "\n",
      "Raw ********************************:\n",
      " inquietum est cor nostrum donec requiescat in te. \"our heart finds no rest until it rests in thee.\" saint augustin is now little more than a celebrated name. outside of learned or theological circles people no longer read him. such is true renown: we admire the saints, as we do great men, on trust. even his is he atoning for the renewal of glory in which he shone during the seventeenth century, when the jansenists, in their inveterate obstinacy, identified him with the defence of their cause? the reputation of sour austerity and of argumentative and tiresome prolixity which attaches to the remembrance of all the writers of portroyal, save pascalhas that affected too the work of augustin, enlisted in spite of himself in the ranks of these pious schismatics? and yet, if there have ever been any beings who do not resemble augustin, and whom probably he would have attacked with all his eloquence and all the force of his dialectic, they are the jansenists. doubtless he would have said with \n",
      "\n",
      "Raw ********************************:\n",
      " the family of dashwood had long been settled in sussex. their estate was large, and their residence was at norland park, in the centre of their property, where, for many generations, they had lived in so respectable a manner as to engage the general good opinion of their surrounding acquaintance. the late owner of this estate was a single man, who lived to a very advanced age, and who for many years of his life, had a constant companion and housekeeper in his sister. but her death, which happened ten years before his own, produced a great alteration in his home; for to supply her loss, he invited and received into his house the family of his nephew mr. henry dashwood, the legal inheritor of the norland estate, and the person to whom he intended to bequeath it. in the society of his nephew and niece, and their children, the old gentleman's days were comfortably spent. his attachment to them all increased. the constant attention of mr. and mrs. henry dashwood to his wishes, which proceed\n",
      "\n",
      "Raw ********************************:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some one has said that if any man would faithfully write his autobiography, giving truly his own history and experiences, the ills and joys, the haps and mishaps that had fallen to his lot, he could not fail to make an interesting story; and disraeli makes sidonia say that there is romance in every life. how much romance, as well as sad reality, there is in the life of a man who, among other experiences, has married seven wives, and has been seven times in prisonsolely on account of the seven wives, may be learned from the pages that follow. i was born in the town of chatham, columbia county, new york, in september, . my father was a new englander, who married three times, and i was the eldest son of his third wife, a woman of dutch descent, or, as she would have boosted if she had been rich, one of the old knickerbockers of new york. my parents were simply honest, hardworking, worthy people, who earned a good livelihood, brought up their children to work, behaved themselves, and were \n",
      "\n",
      "Raw ********************************:\n",
      " the flying ship of professor lucifer sang through the skies like a silver arrow; the bleak white steel of it, gleaming in the bleak blue emptiness of the evening. that it was far above the earth was no expression for it; to the two men in it, it seemed to be far above the stars. the professor had himself invented the flying machine, and had also invented nearly everything in it. every sort of tool or apparatus had, in consequence, to the full, that fantastic and distorted look which belongs to the miracles of science. for the world of science and evolution is far more nameless and elusive and like a dream than the world of poetry and religion; since in the latter images and ideas remain themselves eternally, while it is the whole idea of evolution that identities melt into each other as they do in a nightmare. all the tools of professor lucifer were the ancient human tools gone mad, grown into unrecognizable shapes, forgetful of their origin, forgetful of their names. that thing which \n",
      "\n",
      "Raw ********************************:\n",
      " in reading biographies i always skip the genealogical details. to be born obscure and to die famous has been described as the acme of human felicity. however that may be, whether fame has anything to do with happiness or no, it is a man himself, and not his ancestors, whose life deserves, if it does deserve, to be written. such was froude's own opinion, and it is the opinion of most sensible people. few, indeed, are the families which contain more than one remarkable figure, and this is the rock upon which the hereditary principle always in practice breaks. for human lineage is not subject to the scientific tests which alone could give it solid value as positive or negative evidence. there is nothing to show from what source, other than the ultimate source of every good and perfect gift, froude derived his brilliant and splendid powers. he was a gentleman, and he did not care to find or make for himself a pedigree. he knew that the froudes had been settled in devonshire time out of min\n",
      "\n",
      "Raw ********************************:\n",
      " near the ruins of the castle of rossmore, in ireland, is a small cabin, in which there once lived a widow and her four children. as long as she was able to work, she was very industrious, and was accounted the best spinner in the parish; but she overworked herself at last, and fell ill, so that she could not sit to her wheel as she used to do, and was obliged to give it up to her eldest daughter, mary. mary was at this time about twelve years old. one evening she was sitting at the foot of her mother’s bed spinning, and her little brothers and sisters were gathered round the fire eating their potatoes and milk for supper. “bless them, the poor young creatures!” said the widow, who, as she lay on her bed, which she knew must be her deathbed, was thinking of what would become of her children after she was gone. mary stopped her wheel, for she was afraid that the noise of it had wakened her mother, and would hinder her from going to sleep again. “no need to stop the wheel, mary, dear, for\n",
      "\n",
      "Raw ********************************:\n",
      " william lloyd garrison was born in newburyport, massachusetts, december , . forty years before, daniel palmer, his greatgrandfather, emigrated from massachusetts and settled with three sons and a daughter on the st. john river, in nova scotia. the daughter's name was mary, and it was she who was to be the future grandmother of our hero. one of the neighbors of daniel palmer was joseph garrison, who was probably an englishman. he was certainly a bachelor. the acadian solitude of five hundred acres and mary palmer's charms proved too much for the susceptible heart of joseph garrison. he wooed and won her, and on his thirtieth birthday she became his wife. the bride herself was but twentythree, a woman of resources and of presence of mind, as she needed to be in that primitive settlement. children and cares came apace to the young wife, and we may be sure confined her more and more closely to her house. but in the midst of a fastincreasing family and of multiplying cares a day's outing di\n"
     ]
    }
   ],
   "source": [
    "#Remove the extra whitespace\n",
    "scarlet = ' '.join(scarlet.split())\n",
    "victorians = ' '.join(victorians.split())\n",
    "knights = ' '.join(knights.split())\n",
    "navigator = ' '.join(navigator.split())\n",
    "soldiers = ' '.join(soldiers.split())\n",
    "augustin = ' '.join(augustin.split())\n",
    "sense = ' '.join(sense.split())\n",
    "wives = ' '.join(wives.split())\n",
    "ball = ' '.join(ball.split())\n",
    "froude = ' '.join(froude.split())\n",
    "parents = ' '.join(parents.split())\n",
    "abolitionist = ' '.join(abolitionist.split())\n",
    "\n",
    "#Turn all words into lower case before tokenization\n",
    "scarlet = scarlet.lower()\n",
    "victorians = victorians.lower()\n",
    "knights = knights.lower()\n",
    "navigator = navigator.lower()\n",
    "soldiers = soldiers.lower()\n",
    "augustin = augustin.lower()\n",
    "sense = sense.lower()\n",
    "wives = wives.lower()\n",
    "ball = ball.lower()\n",
    "froude = froude.lower()\n",
    "parents = parents.lower()\n",
    "abolitionist = abolitionist.lower()\n",
    "\n",
    "#Print out texts to see the results\n",
    "titles = [scarlet,victorians,knights,navigator,soldiers,augustin,sense,wives,\n",
    "          ball,froude,parents,abolitionist]\n",
    "for title in titles:\n",
    "    print('\\nRaw ********************************:\\n', title[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarletraw = [scarlet]\n",
    "# tempdf = vectorizer.fit_transform(scarletraw)\n",
    "# s = tempdf.toarray()\n",
    "# scardf = pd.DataFrame(s)\n",
    "# scardf.columns = [vectorizer.get_feature_names()]\n",
    "# scardf.index = ['scar_ArthurDoyle']\n",
    "# scardf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# victoriansraw = [victorians]\n",
    "# tempdf2 = vectorizer.fit_transform(victoriansraw)\n",
    "# v = tempdf2.toarray()\n",
    "# vicdf = pd.DataFrame(v)\n",
    "# vicdf.columns = [vectorizer.get_feature_names()]\n",
    "# vicdf.index = ['vic_LyttonStrachey']\n",
    "# vicdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([scardf, vicdf], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\me-\n",
      "[nltk_data]     fa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Using nltk tokenizer to separate each corpus into sentences\n",
    "\n",
    "scarletsentence = tokenizer.tokenize(scarlet)\n",
    "victorianssentence = tokenizer.tokenize(victorians)\n",
    "knightssentence = tokenizer.tokenize(knights)\n",
    "navigatorsentence = tokenizer.tokenize(navigator)\n",
    "soldierssentence = tokenizer.tokenize(soldiers)\n",
    "augustinsentence = tokenizer.tokenize(augustin)\n",
    "sensesentence = tokenizer.tokenize(sense)\n",
    "wivessentence = tokenizer.tokenize(wives)\n",
    "ballsentence = tokenizer.tokenize(ball)\n",
    "froudsentence = tokenizer.tokenize(froude)\n",
    "parentssentence = tokenizer.tokenize(parents)\n",
    "abolitionistsentence = tokenizer.tokenize(abolitionist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "#Lemmatize Scarlet\n",
    "scarletList = []\n",
    "for sent in scarletsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    scarletList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Victorian\n",
    "vicList = []\n",
    "for sent in victorianssentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    vicList.append(lemma_output)\n",
    "\n",
    "#Lemmatize Knights\n",
    "knightsList = []\n",
    "for sent in knightssentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    knightsList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Navigator\n",
    "naviList = []\n",
    "for sent in navigatorsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    naviList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Soldiers\n",
    "soldierList = []\n",
    "for sent in soldierssentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    soldierList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Augustin\n",
    "augustinList = []\n",
    "for sent in augustinsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    augustinList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Sense\n",
    "senseList = []\n",
    "for sent in sensesentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    senseList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Wives\n",
    "wivesList = []\n",
    "for sent in wivessentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    wivesList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Ball\n",
    "ballList = []\n",
    "for sent in ballsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    ballList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Froude\n",
    "froudList = []\n",
    "for sent in froudsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    froudList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Parents\n",
    "parentList = []\n",
    "for sent in parentssentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    parentList.append(lemma_output)\n",
    "    \n",
    "#Lemmatize Abolitionist\n",
    "aboList = []\n",
    "for sent in abolitionistsentence:\n",
    "    doc = nlp(sent)\n",
    "    lemma_output = \" \".join([token.lemma_ for token in doc])\n",
    "    aboList.append(lemma_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define our vectorizer\n",
    "vectorizer = CountVectorizer(stop_words = {'English'},min_df = 1,\n",
    "                             max_features = 2000,ngram_range = (1,4))\n",
    "# Each section within two star lines does the same thing as the first section.\n",
    "#*****************************************************************************************************\n",
    "scarletdf = pd.DataFrame(scarletList)     #Convert our text into dataframe\n",
    "scarletdf['author'] = 'scar_ArthurDoyle'  #Create an author feature\n",
    "scarletdf.columns = ['sentences','author']   #Rename our columns\n",
    "tempdf = vectorizer.fit_transform(scarletList)   #Fit our data to vectorizer\n",
    "v = tempdf.toarray()   # Convert scipy sparse matric to numpy array so we can see them\n",
    "scarlet_vec = pd.DataFrame(v)   #convert array to panda dataframe\n",
    "scarlet_vec.columns = [vectorizer.get_feature_names()]   #Assigne each feature name as column names\n",
    "scarlet_vec['sentences'] = scarletdf['sentences']   #Combine new word frequency data frame with sentence\n",
    "scarlet_vec['author'] = scarletdf['author']   # and author names, so we can use them as target variable\n",
    "#*****************************************************************************************************\n",
    "vicdf = pd.DataFrame(vicList)            # Same as above\n",
    "vicdf['author'] = 'vic_LyttonStrachey'\n",
    "vicdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(vicList)\n",
    "v = tempdf.toarray()\n",
    "vic_vec = pd.DataFrame(v)\n",
    "vic_vec.columns = [vectorizer.get_feature_names()]\n",
    "vic_vec['sentences'] = vicdf['sentences']\n",
    "vic_vec['author'] = vicdf['author']\n",
    "#*****************************************************************************************************\n",
    "knightsdf = pd.DataFrame(knightsList)         # Same as above\n",
    "knightsdf['author'] = 'kni_AmySteedman'\n",
    "knightsdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(knightsList)\n",
    "v = tempdf.toarray()\n",
    "knights_vec = pd.DataFrame(v)\n",
    "knights_vec.columns = [vectorizer.get_feature_names()]\n",
    "knights_vec['sentences'] = knightsdf['sentences']\n",
    "knights_vec['author'] = knightsdf['author']\n",
    "#*****************************************************************************************************\n",
    "navidf = pd.DataFrame(naviList)              # Same as above\n",
    "navidf['author'] = 'navi_RaymondBeazley'\n",
    "navidf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(naviList)\n",
    "v = tempdf.toarray()\n",
    "navi_vec = pd.DataFrame(v)\n",
    "navi_vec.columns = [vectorizer.get_feature_names()]\n",
    "navi_vec['sentences'] = navidf['sentences']\n",
    "navi_vec['author'] = navidf['author']\n",
    "#*****************************************************************************************************\n",
    "soldierdf = pd.DataFrame(soldierList)              # Same as above\n",
    "soldierdf['author'] = 'sol_RichardDavis'\n",
    "soldierdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(soldierList)\n",
    "v = tempdf.toarray()\n",
    "soldier_vec = pd.DataFrame(v)\n",
    "soldier_vec.columns = [vectorizer.get_feature_names()]\n",
    "soldier_vec['sentences'] = soldierdf['sentences']\n",
    "soldier_vec['author'] = soldierdf['author']\n",
    "#*****************************************************************************************************\n",
    "augustindf = pd.DataFrame(augustinList)              # Same as above\n",
    "augustindf['author'] = 'aug_LouisBertrand'\n",
    "augustindf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(augustinList)\n",
    "v = tempdf.toarray()\n",
    "augustin_vec = pd.DataFrame(v)\n",
    "augustin_vec.columns = [vectorizer.get_feature_names()]\n",
    "augustin_vec['sentences'] = augustindf['sentences']\n",
    "augustin_vec['author'] = augustindf['author']\n",
    "#*****************************************************************************************************\n",
    "sensedf = pd.DataFrame(senseList)              # Same as above\n",
    "sensedf['author'] = 'sense_JaneAusten'\n",
    "sensedf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(senseList)\n",
    "v = tempdf.toarray()\n",
    "sense_vec = pd.DataFrame(v)\n",
    "sense_vec.columns = [vectorizer.get_feature_names()]\n",
    "sense_vec['sentences'] = sensedf['sentences']\n",
    "sense_vec['author'] = sensedf['author']\n",
    "#*****************************************************************************************************\n",
    "wivesdf = pd.DataFrame(wivesList)              # Same as above\n",
    "wivesdf['author'] = 'wives_LAAbbott'\n",
    "wivesdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(wivesList)\n",
    "v = tempdf.toarray()\n",
    "wives_vec = pd.DataFrame(v)\n",
    "wives_vec.columns = [vectorizer.get_feature_names()]\n",
    "wives_vec['sentences'] = wivesdf['sentences']\n",
    "wives_vec['author'] = wivesdf['author']\n",
    "#*****************************************************************************************************\n",
    "balldf = pd.DataFrame(ballList)              # Same as above\n",
    "balldf['author'] = 'ball_GKChesterton'\n",
    "balldf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(ballList)\n",
    "v = tempdf.toarray()\n",
    "ball_vec = pd.DataFrame(v)\n",
    "ball_vec.columns = [vectorizer.get_feature_names()]\n",
    "ball_vec['sentences'] = balldf['sentences']\n",
    "ball_vec['author'] = balldf['author']\n",
    "#*****************************************************************************************************\n",
    "froudedf = pd.DataFrame(froudList)              # Same as above\n",
    "froudedf['author'] = 'frou_HerbertPaul'\n",
    "froudedf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(froudList)\n",
    "v = tempdf.toarray()\n",
    "froude_vec = pd.DataFrame(v)\n",
    "froude_vec.columns = [vectorizer.get_feature_names()]\n",
    "froude_vec['sentences'] = froudedf['sentences']\n",
    "froude_vec['author'] = froudedf['author']\n",
    "#*****************************************************************************************************\n",
    "parentdf = pd.DataFrame(parentList)              # Same as above\n",
    "parentdf['author'] = 'pare_MariaEdgeworth'\n",
    "parentdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(parentList)\n",
    "v = tempdf.toarray()\n",
    "parent_vec = pd.DataFrame(v)\n",
    "parent_vec.columns = [vectorizer.get_feature_names()]\n",
    "parent_vec['sentences'] = parentdf['sentences']\n",
    "parent_vec['author'] = parentdf['author']\n",
    "#*****************************************************************************************************\n",
    "abolitionistdf = pd.DataFrame(aboList)              # Same as above\n",
    "abolitionistdf['author'] = 'abo_ArchibaldGrimke'\n",
    "abolitionistdf.columns = ['sentences','author']\n",
    "tempdf = vectorizer.fit_transform(aboList)\n",
    "v = tempdf.toarray()\n",
    "abolitionist_vec = pd.DataFrame(v)\n",
    "abolitionist_vec.columns = [vectorizer.get_feature_names()]\n",
    "abolitionist_vec['sentences'] = abolitionistdf['sentences']\n",
    "abolitionist_vec['author'] = abolitionistdf['author']\n",
    "#*****************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43951, 8693)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([scarlet_vec,vic_vec,knights_vec,navi_vec,soldier_vec,\n",
    "                   augustin_vec,sense_vec,wives_vec,ball_vec,froude_vec,\n",
    "                   parent_vec,abolitionist_vec], axis = 0)\n",
    "\n",
    "col_levels = list(train_df.columns.levels)\n",
    "train_df.columns = list(col_levels[0])\n",
    "train_df = train_df.fillna(0)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df.author == 'scar_ArthurDoyle', 'author_number'] = 1\n",
    "train_df.loc[train_df.author == 'vic_LyttonStrachey', 'author_number'] = 2\n",
    "train_df.loc[train_df.author == 'kni_AmySteedman', 'author_number'] = 3\n",
    "train_df.loc[train_df.author == 'navi_RaymondBeazley', 'author_number'] = 4\n",
    "train_df.loc[train_df.author == 'sol_RichardDavis', 'author_number'] = 5\n",
    "train_df.loc[train_df.author == 'aug_LouisBertrand', 'author_number'] = 6\n",
    "train_df.loc[train_df.author == 'sense_JaneAusten', 'author_number'] = 7\n",
    "train_df.loc[train_df.author == 'wives_LAAbbott', 'author_number'] = 8\n",
    "train_df.loc[train_df.author == 'ball_GKChesterton', 'author_number'] = 9\n",
    "train_df.loc[train_df.author == 'frou_HerbertPaul', 'author_number'] = 10\n",
    "train_df.loc[train_df.author == 'pare_MariaEdgeworth', 'author_number'] = 11\n",
    "train_df.loc[train_df.author == 'abo_ArchibaldGrimke', 'author_number'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "Y = train_df['author_number'].values.ravel()\n",
    "X = train_df.drop(['author','sentences','author_number'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Component-wise and Cumulative Explained Variance')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=800)\n",
    "pca_result = pca.fit_transform(train_df.drop(['author','sentences','author_number'], axis = 1).T)\n",
    "plt.plot(range(800), pca.explained_variance_ratio_)\n",
    "plt.plot(range(800), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component-wise and Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(rfc, X, Y, cv=5, n_jobs=-1, scoring = 'accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43951, 8694)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 8691)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 13,\n",
    "    'gamma': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'lambda': 2,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 6,\n",
    "    'silent': 1,\n",
    "    'seed': 1000\n",
    "}\n",
    "\n",
    "plst = params.items()\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "num_rounds = 500\n",
    "model = xgb.train(plst, dtrain, num_rounds)\n",
    "\n",
    "# 对测试集进行预测\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "ans = model.predict(dtest)\n",
    "\n",
    "# 计算准确率\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "for i in range(len(y_test)):\n",
    "    if ans[i] == y_test[i]:\n",
    "        cnt1 += 1\n",
    "    else:\n",
    "        cnt2 += 1\n",
    "\n",
    "print(\"Accuracy: %.2f %% \" % (100 * cnt1 / (cnt1 + cnt2)))\n",
    "\n",
    "# 显示重要特征\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9860447478194918\n",
      "\n",
      "Test set score: 0.46885842671065353\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39022172 0.45758472 0.44516496 0.42910787 0.42520492]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(rfc, X, Y, cv=5, n_jobs=-1, scoring = 'accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(mlp, X, Y, cv=5, n_jobs=-1, scoring = 'accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbors = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "neighbors.fit(X_train, y_train.values.ravel())\n",
    "print('Training set score:', neighbors.score(X_train, y_train))\n",
    "print('\\nTest set score:', neighbors.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en')\n",
    "\n",
    "# # All the processing work is done here, so it may take a while.\n",
    "# scarlet_doc = nlp(scarlet)\n",
    "# victorians_doc = nlp(victorians)\n",
    "# knights_doc = nlp(knights)\n",
    "# navigator_doc = nlp(navigator)\n",
    "# soldiers_doc = nlp(soldiers)\n",
    "# augustin_doc = nlp(augustin)\n",
    "# sense_doc = nlp(sense)\n",
    "# wives_doc = nlp(wives)\n",
    "# ball_doc = nlp(ball)\n",
    "# froude_doc = nlp(froude)\n",
    "# parents_doc = nlp(parents)\n",
    "# abolitionist_doc = nlp(abolitionist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group into sentences.\n",
    "# scarlet_sents = [[sent, \"scar_ArthurDoyle\"] for sent in scarlet_doc.sents]\n",
    "# victorians_sents = [[sent, \"vic_LyttonStrachey\"] for sent in victorians_doc.sents]\n",
    "# knights_sents = [[sent, \"kni_AmySteedman\"] for sent in knights_doc.sents]\n",
    "# navigator_sents = [[sent, \"navi_RaymondBeazley\"] for sent in navigator_doc.sents]\n",
    "# soldiers_sents = [[sent, \"sol_RichardDavis\"] for sent in soldiers_doc.sents]\n",
    "# augustin_sents = [[sent, \"aug_LouisBertrand\"] for sent in augustin_doc.sents]\n",
    "# sense_sents = [[sent, \"sense_JaneAusten\"] for sent in sense_doc.sents]\n",
    "# wives_sents = [[sent, \"wives_LAAbbott\"] for sent in wives_doc.sents]\n",
    "# ball_sents = [[sent, \"ball_GKChesterton\"] for sent in ball_doc.sents]\n",
    "# froude_sents = [[sent, \"frou_HerbertPaul\"] for sent in froude_doc.sents]\n",
    "# parents_sents = [[sent, \"pare_MariaEdgeworth\"] for sent in parents_doc.sents]\n",
    "# abolitionist_sents = [[sent, \"abo_ArchibaldGrimke\"] for sent in abolitionist_doc.sents]\n",
    "\n",
    "# # Combine the sentences from the two novels into one data frame.\n",
    "# sentences = pd.DataFrame(scarlet_sents + victorians_sents + knights_sents + navigator_sents \n",
    "#                          + soldiers_sents + augustin_sents + sense_sents + wives_sents + ball_sents \n",
    "#                          + froude_sents + parents_sents + abolitionist_sents)\n",
    "# sentences.columns = ['sentences','authur']\n",
    "# sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scarlet_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 43042 tokens long\n",
      "The first three tokens are 'being a reprint from the reminiscences of john h. watson, m.d., late of the army medical department. in the year 1878 i took my degree of doctor of medicine of the university of london, and proceeded to netley to go through the course prescribed for surgeons in the army. having completed my studies there, i was duly attached to the fifth northumberland fusiliers as assistant surgeon. the regiment was stationed in india at the time, and before i could join it, the second afghan war had broken out. on'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The scarlet_doc object is a {} object.\".format(type(scarlet_doc)))\n",
    "print(\"It is {} tokens long\".format(len(scarlet_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(scarlet_doc[:100]))\n",
    "print(\"The type of each token is {}\".format(type(scarlet_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: [('I', 1984), (\"'s\", 698), ('Elinor', 679), ('Marianne', 564), ('Mrs.', 526), ('said', 392), ('But', 283), ('sister', 280), ('mother', 257), ('She', 257)]\n",
      "Parents Assistant: [('I', 3611), (\"'s\", 1425), ('said', 1425), (\"n't\", 578), ('The', 541), ('Mr.', 454), ('good', 423), ('know', 415), ('little', 400), (\"'ll\", 329)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "\n",
    "# Use our optional keyword argument to remove stop words.\n",
    "sense_freq = word_frequencies(sense_doc, include_stop=False).most_common(10)\n",
    "parents_freq = word_frequencies(parents_doc, include_stop=False).most_common(10)\n",
    "print('Sense:', sense_freq)\n",
    "print('Parents Assistant:', parents_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Sense: {'mother', 'Elinor', 'But', 'Mrs.', 'Marianne', 'sister', 'She'}\n",
      "Unique to Emma: {'good', 'little', 'know', \"'ll\", 'The', 'Mr.', \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "sense_common = [pair[0] for pair in sense_freq]\n",
    "parents_common = [pair[0] for pair in parents_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Sense:', set(sense_common) - set(parents_common))\n",
    "print('Unique to Emma:', set(parents_common) - set(sense_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sense: [('-PRON-', 3184), (\"'s\", 684), ('elinor', 680), ('marianne', 564), ('mrs.', 526), ('say', 443), ('know', 384), ('think', 329), ('sister', 328), ('but', 283)]\n",
      "Parents Assistant: [('-PRON-', 5028), ('say', 1531), (\"'s\", 927), ('be', 822), ('good', 642), ('not', 636), ('come', 633), ('know', 598), ('the', 577), ('will', 478)]\n",
      "Unique to Sense: {'marianne', 'mrs.', 'but', 'think', 'sister', 'elinor'}\n",
      "Unique to Parents: {'good', 'will', 'be', 'come', 'the', 'not'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "sense_lemma_freq = lemma_frequencies(sense_doc, include_stop=False).most_common(10)\n",
    "parents_lemma_freq = lemma_frequencies(parents_doc, include_stop=False).most_common(10)\n",
    "print('\\nSense:', sense_lemma_freq)\n",
    "print('Parents Assistant:', parents_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "sense_lemma_common = [pair[0] for pair in sense_lemma_freq]\n",
    "parents_lemma_common = [pair[0] for pair in parents_lemma_freq]\n",
    "print('Unique to Sense:', set(sense_lemma_common) - set(parents_lemma_common))\n",
    "print('Unique to Parents:', set(parents_lemma_common) - set(sense_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense has 5139 sentences.\n",
      "Here is an example: \n",
      "The late owner of this estate was a single man, who lived to a very advanced age, and who for many years of his life, had a constant companion and housekeeper in his sister.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(sense_doc.sents)\n",
    "print(\"Sense has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 words in this sentence, and 28 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence.\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "The DET\n",
      "late ADJ\n",
      "owner NOUN\n",
      "of ADP\n",
      "this DET\n",
      "estate NOUN\n",
      "was VERB\n",
      "a DET\n",
      "single ADJ\n"
     ]
    }
   ],
   "source": [
    "# View the part of speech for some tokens in our sentence.\n",
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "The det owner\n",
      "late amod owner\n",
      "owner nsubj was\n",
      "of prep owner\n",
      "this det estate\n",
      "estate pobj of\n",
      "was ROOT was\n",
      "a det man\n",
      "single amod man\n"
     ]
    }
   ],
   "source": [
    "# View the dependencies for some tokens.\n",
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG Dashwood\n",
      "PERSON Sussex\n",
      "LOC Norland Park\n",
      "DATE many years\n",
      "DATE ten years\n",
      "PERSON Henry Dashwood\n",
      "ORG Gentleman\n",
      "DATE days\n",
      "PERSON Henry Dashwood\n",
      "PERSON Henry Dashwood\n"
     ]
    }
   ],
   "source": [
    "# Extract the first ten entities.\n",
    "entities = list(sense_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Lady Middleton's\", 'Ferrars DID', 'Fanny', 'Rose', 'Lucy', 'Margaret', 'May', 'Impudence', 'Pope', 'Folly', 'gum--', 'John', 'Taylor', 'Dearest Marianne', \"Miss Grey's\", 'Richardsons', 'Harris', 'all.--', 'Robert Ferrars', 'Marianne come?\"-- Elinor', 'be.--', 'Cruel', 'Constantia', 'Miss Lucy--', 'William', 'Queen Mab', 'these:--', 'Miss Lucy', 'Avignon', 'John Willoughby', 'Whitwell', 'Miss Dashwoods', 'saying,--', 'Middletons', 'Sally', 'Whom', 'Thomas Palmer', 'Astonishment', 'Edward Ferrars', 'Dashwood WOULD', 'Anne', 'Jenning', 'Willoughby!\"--', 'Miss Morton', 'Willoughby', 'Scott', 'Down', 'Combe Magna', 'Biddy Henshawe', 'Palmer', 'EDWARD Ferrars', 'Dashwood', 'Hush', 'Sussex', 'Lucy Steele', 'Betty', 'Clarke', 'Richardson', 'Miss Godby', 'Law', 'Poor Anne', 'Smith', 'Gray', 'Harry', 'Edward', 'Pratt', 'Ma', 'DID', 'Shakespeare', 'Grey', 'said,--', 'JOHN WILLOUGHBY', \"Lady Middletons's\", 'John Smith', 'Marianne NOW', 'Ferrars', 'Simpson', 'Elinor', 'Annamaria', 'Whatever Marianne', 'Smith--', 'Gibson', 'Cartwright', 'Miss Sparks', 'Edward?--', 'Miss Walker', 'Richard', 'Columella', 'much;--', 'John Middleton', 'Dennison', 'Charlotte.--\"And', 'Exeter', 'Grandeur', 'Lady Elliott', 'rupture:-- Edward', 'Sophia', 'Twill', 'Henry Dashwood', 'Marianne Dashwood', 'Norland', 'Misses Dashwood', 'Jennings', 'Hum\"--', 'Miss Marianne', 'Martha Sharpe', 'Steele', 'Ellisons', 'back!--Lord', 'Morton', 'Eliza Williams', 'Eliza', 'Miss Dashwood', 'Amongst', \"Miss Dashwood's\", 'Palmer--\"then', 'Lady Middleton', 'John Dashwood', 'Courtland', 'Marianne', 'La', 'Weymouth', 'My Elinor', 'God', 'Mary', 'her--', 'Donavan', 'Holburn', 'Barton', \"Miss Morton's\", 'Robert', 'ROBERT Ferrars', 'Davies', 'died,--', 'Harry Dashwood', 'Nancy', 'Ellison', 'Thomas', 'mine--', 'so?--', 'Whatever Lucy', 'GAUCHERIE', 'Miss Grey', 'Mary Brown', 'Burgess', 'Norland Common', 'Cowper', 'Mrs', 'me.--', 'Brandon', 'Reflection', 'myself\"--', 'Miss Steele', 'Exert', 'Miss Williams', 'Elliott', 'Bartlett', 'voice,--', 'Robert Ferrars!\"--was'}\n"
     ]
    }
   ],
   "source": [
    "# All of the uniqe entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(sense_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "parents_sents = [[sent, \"Maria\"] for sent in parents_doc.sents]\n",
    "sense_sents = sense_sents[0:2000]\n",
    "parents_sents = parents_sents[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(The, family, of, Dashwood, had, long, been, s...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Their, estate, was, large, ,, and, their, res...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(The, late, owner, of, this, estate, was, a, s...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(But, her, death, ,, which, happened, ten, yea...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(In, the, society, of, his, nephew, and, niece...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (The, family, of, Dashwood, had, long, been, s...  Austen\n",
       "1  (Their, estate, was, large, ,, and, their, res...  Austen\n",
       "2  (The, late, owner, of, this, estate, was, a, s...  Austen\n",
       "3  (But, her, death, ,, which, happened, ten, yea...  Austen\n",
       "4  (In, the, society, of, his, nephew, and, niece...  Austen"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(sense_sents + parents_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(100)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(sensewords + parentswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>place</th>\n",
       "      <th>mother</th>\n",
       "      <th>john</th>\n",
       "      <th>suppose</th>\n",
       "      <th>hope</th>\n",
       "      <th>edward</th>\n",
       "      <th>tell</th>\n",
       "      <th>go</th>\n",
       "      <th>sir</th>\n",
       "      <th>...</th>\n",
       "      <th>give</th>\n",
       "      <th>speak</th>\n",
       "      <th>world</th>\n",
       "      <th>madam</th>\n",
       "      <th>child</th>\n",
       "      <th>year</th>\n",
       "      <th>moment</th>\n",
       "      <th>dear</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, family, of, Dashwood, had, long, been, s...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Their, estate, was, large, ,, and, their, res...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, late, owner, of, this, estate, was, a, s...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, her, death, ,, which, happened, ten, yea...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, the, society, of, his, nephew, and, niece...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  family place mother john suppose hope edward tell go sir     ...     give  \\\n",
       "0      1     0      0    0       0    0      0    0  0   0     ...        0   \n",
       "1      0     0      0    0       0    0      0    0  0   0     ...        0   \n",
       "2      0     0      0    0       0    0      0    0  0   0     ...        0   \n",
       "3      1     0      0    0       0    0      0    0  0   0     ...        0   \n",
       "4      0     0      0    0       0    0      0    0  0   0     ...        0   \n",
       "\n",
       "  speak world madam child year moment dear  \\\n",
       "0     0     0     0     0    0      0    0   \n",
       "1     0     0     0     0    0      0    0   \n",
       "2     0     0     0     0    1      0    0   \n",
       "3     0     0     0     0    1      0    0   \n",
       "4     0     0     0     1    0      0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (The, family, of, Dashwood, had, long, been, s...      Austen  \n",
       "1  (Their, estate, was, large, ,, and, their, res...      Austen  \n",
       "2  (The, late, owner, of, this, estate, was, a, s...      Austen  \n",
       "3  (But, her, death, ,, which, happened, ten, yea...      Austen  \n",
       "4  (In, the, society, of, his, nephew, and, niece...      Austen  \n",
       "\n",
       "[5 rows x 149 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9241666666666667\n",
      "\n",
      "Test set score: 0.7625\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 147) (2400,)\n",
      "Training set score: 0.815\n",
      "\n",
      "Test set score: 0.78125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8054166666666667\n",
      "\n",
      "Test set score: 0.77625\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
